Initial thought regarding human language: When to abstract primitive definition into a representative word?

Parallel: Compilers have 'Abstract Syntax Trees' that translate natural language into optimal machine code.

Our subconscious has the abstract syntax tree that infers the primitive meaning of high-level words in milliseconds.

I wonder how that is done artificially?

1. Vector Embeddings: Possible because companies had people arbitrarily label content with a 'relevancy magnitude' (distance for cosine similarity), then allowed a model to infer to the point where it can be considered 'pre-trained' or did the relationship between the alphabet and numbers from ASCII have a semantic foundation researchers used?